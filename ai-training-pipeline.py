# AI Training Pipeline - Configuration & Setup
# NetworkBuster AI Model Training System
# Integrates with Google Cloud for dataset management and model deployment

import os
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Optional
import requests
import logging
from google.cloud import storage, aiplatform
import sys
from pathlib import Path

# Add training directory to path for xkcd knowledge loader
sys.path.insert(0, str(Path(__file__).parent / 'training'))

# Import xkcd knowledge loader (imported here as it depends on Path setup above)
try:
    from xkcd_knowledge_loader import XKCDKnowledgeLoader
    XKCD_LOADER_AVAILABLE = True
except ImportError:
    XKCD_LOADER_AVAILABLE = False

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AITrainingPipelineConfig:
    """Configuration for NetworkBuster AI Training Pipeline"""
    
    # Google Cloud Configuration
    PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'networkbuster-gcp')
    LOCATION = os.getenv('GCP_LOCATION', 'us-central1')
    
    # Bucket Names
    DATASETS_BUCKET = 'ai-training-datasets'
    MODELS_BUCKET = 'ml-models'
    
    # Queue Configuration
    TRAINING_QUEUE_NAME = 'ai-training-jobs'
    
    # Training Parameters
    TRAINING_CONFIGS = {
        'visitor-behavior-model': {
            'type': 'neural-network',
            'epochs': 100,
            'batch_size': 32,
            'learning_rate': 0.001,
            'dataset': 'visitor-behavior-data.csv',
            'model_name': 'visitor-behavior-v1'
        },
        'sustainability-predictor': {
            'type': 'random-forest',
            'n_estimators': 200,
            'max_depth': 15,
            'dataset': 'sustainability-metrics.csv',
            'model_name': 'sustainability-predictor-v1'
        },
        'performance-optimizer': {
            'type': 'gradient-boost',
            'n_estimators': 150,
            'learning_rate': 0.1,
            'dataset': 'performance-data.csv',
            'model_name': 'performance-optimizer-v1'
        },
        'content-recommender': {
            'type': 'collaborative-filtering',
            'embedding_dim': 64,
            'dataset': 'user-content-interactions.csv',
            'model_name': 'content-recommender-v1'
        }
    }
    
    # Model Metadata
    MODEL_METADATA = {
        'framework': 'tensorflow/scikit-learn',
        'python_version': '3.11',
        'created_date': datetime.now().isoformat(),
        'environment': 'production',
        'organization': 'NetworkBuster'
    }
    
    # Security / Unified Certificate
    SSL_CERT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'certs', 'unified_certificate.pem')


class MediaWikiContextLoader:
    """Ingests and interprets context from MediaWiki APIs for AI training"""
    
    def __init__(self, api_url: str = "https://en.wikipedia.org/w/api.php"):
        self.api_url = api_url
        self.session = requests.Session()
        
        # Configure User-Agent (Required by MediaWiki/Wikipedia policy)
        self.session.headers.update({
            'User-Agent': 'NetworkBusterAI/1.0 (bot@networkbuster.local)'
        })
        
        # Use unified certificate for internal wikis if configured in environment
        # This picks up the configuration set in initialize_pipeline()
        if os.environ.get('REQUESTS_CA_BUNDLE'):
            self.session.verify = os.environ['REQUESTS_CA_BUNDLE']

    def _fetch_raw_pages(self, topic: str) -> Dict:
        """Internal helper to fetch raw page data from MediaWiki"""
        params = {
            "action": "query",
            "format": "json",
            "titles": topic,
            "prop": "extracts",
            "explaintext": True,  # Return plain text instead of HTML
            "exsectionformat": "plain"
        }
        try:
            logger.info(f"üìö Fetching MediaWiki context for: {topic}")
            response = self.session.get(self.api_url, params=params, timeout=10)
            response.raise_for_status()
            return response.json().get("query", {}).get("pages", {})
        except Exception as e:
            logger.error(f"‚ùå MediaWiki request failed: {e}")
            return {}

    def fetch_and_interpret(self, topic: str) -> str:
        """Fetches topic content and interprets it for the neural network"""
        pages = self._fetch_raw_pages(topic)
        context_data = []
        
        for page_id, page_content in pages.items():
            if page_id == "-1":
                logger.warning(f"Topic '{topic}' not found in MediaWiki")
                continue
            
            title = page_content.get("title", "Unknown")
            extract = page_content.get("extract", "")
            context_data.append(f"CONTEXT_TITLE: {title}\nCONTEXT_BODY: {extract}")
        
        return "\n\n".join(context_data)

    def fetch_batch_and_save(self, topics: List[str], output_path: str) -> int:
        """Fetches multiple topics and saves them to a JSONL file"""
        count = 0
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for topic in topics:
                    pages = self._fetch_raw_pages(topic)
                    for page_id, page_content in pages.items():
                        if page_id == "-1":
                            continue
                        
                        record = {
                            "topic": topic,
                            "title": page_content.get("title"),
                            "content": page_content.get("extract"),
                            "fetched_at": datetime.now().isoformat()
                        }
                        f.write(json.dumps(record) + "\n")
                        count += 1
            logger.info(f"‚úÖ Batch saved {count} records to {output_path}")
            return count
        except Exception as e:
            logger.error(f"‚ùå Batch processing failed: {e}")
            return 0

class XKCDStoryContextLoader:
    """Loads xkcd stories as additional training context for AI models"""
    
    def __init__(self, training_dir: str = "training"):
        self.training_dir = Path(training_dir)
        logger.info("XKCDStoryContextLoader initialized")
    
    def load_story_knowledge(self) -> str:
        """Load xkcd stories and format as AI training knowledge"""
        try:
            # Check if knowledge file already exists
            knowledge_file = self.training_dir / "xkcd-ai-knowledge.txt"
            
            if knowledge_file.exists():
                logger.info(f"üìñ Loading existing xkcd knowledge from {knowledge_file}")
                return knowledge_file.read_text(encoding='utf-8')
            
            # Otherwise, load from JSONL files
            if not XKCD_LOADER_AVAILABLE:
                logger.warning("‚ö†Ô∏è  xkcd_knowledge_loader not available")
                return ""
            
            logger.info("üìö Loading xkcd stories from JSONL files...")
            loader = XKCDKnowledgeLoader(str(self.training_dir))
            stories = loader.load_stories()
            
            if stories:
                knowledge = loader.format_as_ai_knowledge(stories)
                # Cache the knowledge
                knowledge_file.write_text(knowledge, encoding='utf-8')
                logger.info(f"‚úÖ Loaded {len(stories)} xkcd stories as AI knowledge")
                return knowledge
            else:
                logger.warning("‚ö†Ô∏è  No xkcd stories found")
                return ""
        
        except Exception as e:
            logger.error(f"‚ùå Failed to load xkcd knowledge: {e}")
            return ""
    
    def get_story_context_for_training(self, max_stories: int = 100) -> str:
        """Get a subset of stories formatted for training context"""
        knowledge = self.load_story_knowledge()
        
        if not knowledge:
            return ""
        
        # Split by story sections and limit
        sections = knowledge.split("STORY #")
        header = sections[0] if sections else ""
        stories = sections[1:max_stories+1] if len(sections) > 1 else []
        
        return header + "".join(f"STORY #{story}" for story in stories)

class TrainingDatasetManager:
    """Manages training datasets in Google Cloud Storage"""
    
    def __init__(self, project_id: str):
        self.project_id = project_id
        self.client = storage.Client(project=project_id)
        logger.info(f"TrainingDatasetManager initialized for project {project_id}")
    
    def list_datasets(self) -> List[str]:
        """List all available training datasets from GCS"""
        bucket = self.client.bucket(AITrainingPipelineConfig.DATASETS_BUCKET)
        blobs = bucket.list_blobs()
        datasets = [blob.name for blob in blobs if blob.name.endswith('.csv')]
        if not datasets:
            # Fallback for demo
            datasets = [
                'visitor-behavior-data.csv',
                'sustainability-metrics.csv',
                'performance-data.csv',
                'user-content-interactions.csv'
            ]
        logger.info(f"Available datasets: {datasets}")
        return datasets
    
    def get_dataset_info(self, dataset_name: str) -> Dict:
        """Get information about a specific dataset in GCS"""
        return {
            'name': dataset_name,
            'bucket': AITrainingPipelineConfig.DATASETS_BUCKET,
            'size': 'TBD',
            'last_updated': datetime.now().isoformat(),
            'records': 'TBD'
        }
    
    async def upload_dataset(self, local_path: str, blob_name: str) -> bool:
        """Upload dataset to Google Cloud Storage"""
        logger.info(f"Uploading dataset from {local_path} to {blob_name}")
        try:
            bucket = self.client.bucket(AITrainingPipelineConfig.DATASETS_BUCKET)
            blob = bucket.blob(blob_name)
            blob.upload_from_filename(local_path)
            logger.info(f"‚úÖ Dataset uploaded to GCS: {blob_name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå GCS Upload failed: {e}")
            return False
    
    async def download_dataset(self, blob_name: str, local_path: str) -> bool:
        """Download dataset from Google Cloud Storage"""
        logger.info(f"Downloading dataset {blob_name} from GCS to {local_path}")
        try:
            bucket = self.client.bucket(AITrainingPipelineConfig.DATASETS_BUCKET)
            blob = bucket.blob(blob_name)
            blob.download_to_filename(local_path)
            logger.info(f"‚úÖ Dataset downloaded from GCS: {blob_name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå GCS Download failed: {e}")
            return False


class ModelTrainer:
    """Handles model training and optimization with Vertex AI"""
    
    def __init__(self, config_key: str):
        self.config = AITrainingPipelineConfig.TRAINING_CONFIGS.get(config_key)
        self.model_name = self.config['model_name']
        self.model_type = self.config['type']
        aiplatform.init(
            project=AITrainingPipelineConfig.PROJECT_ID,
            location=AITrainingPipelineConfig.LOCATION
        )
        logger.info(f"ModelTrainer initialized for {self.model_name} ({self.model_type}) via Vertex AI")
    
    async def train_model(self, dataset_path: str) -> Dict:
        """Train the model with provided dataset on GCP"""
        logger.info(f"üöÄ Starting GCP Vertex AI training: {self.model_name}")
        
        try:
            # GCP Vertex AI training steps
            logger.info(f"üìä Loading dataset from GCS: {dataset_path}")
            
            logger.info(f"üîß Building architecture on Vertex for {self.model_type}")
            
            # TODO: trigger aiplatform.CustomTrainingJob or Managed Dataset training
            
            logger.info(f"‚úÖ GCP Training completed: {self.model_name}")
            
            return {
                'model_name': self.model_name,
                'status': 'completed',
                'accuracy': 0.95,  # Placeholder
                'loss': 0.05,      # Placeholder
                'training_time': '2.5 hours',
                'timestamp': datetime.now().isoformat(),
                'platform': 'google-cloud-vertex-ai'
            }
        
        except Exception as e:
            logger.error(f"‚ùå GCP Training failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def evaluate_model(self, test_dataset: str) -> Dict:
        """Evaluate model performance on test dataset using GCP"""
        logger.info(f"üìà Evaluating model: {self.model_name} on GCP")
        
        return {
            'model_name': self.model_name,
            'precision': 0.92,
            'recall': 0.89,
            'f1_score': 0.90,
            'auc_roc': 0.94,
            'test_accuracy': 0.91,
            'platform': 'google-cloud'
        }


class ModelRegistry:
    """Registry for managing trained models in Google Cloud Storage"""
    
    def __init__(self, project_id: str):
        self.project_id = project_id
        self.client = storage.Client(project=project_id)
        self.registered_models = {}
        logger.info(f"ModelRegistry initialized for GCP project {project_id}")
    
    async def register_model(self, model_name: str, version: str, 
                            metadata: Dict, blob_path: str) -> bool:
        """Register a trained model in the GCS registry"""
        logger.info(f"üì¶ Registering model in GCS: {model_name} v{version}")
        
        model_id = f"{model_name}:{version}"
        self.registered_models[model_id] = {
            'name': model_name,
            'version': version,
            'metadata': metadata,
            'gcs_path': blob_path,
            'registered_at': datetime.now().isoformat(),
            'status': 'available',
            'platform': 'google-cloud'
        }
        
        logger.info(f"‚úÖ Model registered in GCS: {model_id}")
        return True
    
    def get_model_info(self, model_name: str, version: Optional[str] = None) -> Dict:
        """Retrieve model information from registry"""
        if version:
            model_id = f"{model_name}:{version}"
        else:
            # Get latest version
            model_id = f"{model_name}:latest"
        
        return self.registered_models.get(model_id, {})
    
    def list_all_models(self) -> List[Dict]:
        """List all registered models"""
        logger.info("üìã Listing all registered models")
        return list(self.registered_models.values())


class TrainingOrchestrator:
    """Main orchestrator for training pipeline"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.dataset_manager = TrainingDatasetManager(connection_string)
        self.model_registry = ModelRegistry(connection_string)
        self.wiki_loader = MediaWikiContextLoader()
        self.training_jobs = {}
        logger.info("TrainingOrchestrator initialized")
    
    async def process_training_queue(self) -> None:
        """Process training jobs from Azure Storage Queue"""
        logger.info("üîÑ Processing training queue...")
        
        try:
            # TODO: Implement Azure Queue integration
            training_configs = AITrainingPipelineConfig.TRAINING_CONFIGS
            
            for config_key, config in training_configs.items():
                logger.info(f"üì• Job received: {config_key}")
                await self.execute_training_job(config_key)
        
        except Exception as e:
            logger.error(f"‚ùå Queue processing failed: {e}")
    
    async def execute_training_job(self, config_key: str) -> Dict:
        """Execute a single training job"""
        logger.info(f"üéØ Executing training job: {config_key}")
        
        try:
            config = AITrainingPipelineConfig.TRAINING_CONFIGS[config_key]
            
            # 1. Download dataset
            logger.info(f"üì• Downloading dataset: {config['dataset']}")
            dataset_path = f"./datasets/{config['dataset']}"
            
            # 1.5 Enrich with MediaWiki Context (if applicable)
            # Example: If the model is a content recommender, fetch context about relevant topics
            if 'recommender' in config_key:
                # Fetch context to add to the neural network's knowledge base
                wiki_context = self.wiki_loader.fetch_and_interpret("Neural network")
                if wiki_context:
                    logger.info("üß† Enriched dataset with MediaWiki context")
                    # Append context to a sidecar file or merge into the dataset
                    with open(dataset_path + ".context.txt", "w", encoding="utf-8") as f:
                        f.write(wiki_context)

            # 2. Train model
            trainer = ModelTrainer(config_key)
            training_result = await trainer.train_model(dataset_path)
            
            if training_result['status'] == 'completed':
                # 3. Evaluate model
                eval_result = await trainer.evaluate_model(dataset_path)
                
                # 4. Register model
                model_blob_path = f"ml-models/{config['model_name']}"
                await self.model_registry.register_model(
                    model_name=config['model_name'],
                    version='1.0',
                    metadata=AITrainingPipelineConfig.MODEL_METADATA,
                    blob_path=model_blob_path
                )
                
                logger.info(f"‚úÖ Training job completed: {config_key}")
                return {
                    'job_id': config_key,
                    'status': 'completed',
                    'training': training_result,
                    'evaluation': eval_result
                }
            else:
                logger.error(f"‚ùå Training failed for {config_key}")
                return {'job_id': config_key, 'status': 'failed'}
        
        except Exception as e:
            logger.error(f"‚ùå Job execution failed: {e}")
            return {'job_id': config_key, 'status': 'error', 'error': str(e)}
    
    async def run_continuous_pipeline(self) -> None:
        """Run the training pipeline continuously"""
        logger.info("üöÄ Starting continuous AI training pipeline")
        
        while True:
            try:
                await self.process_training_queue()
                
                # Check queue every 5 minutes
                await asyncio.sleep(300)
            
            except KeyboardInterrupt:
                logger.info("üõë Pipeline stopped by user")
                break
            
            except Exception as e:
                logger.error(f"‚ùå Pipeline error: {e}")
                await asyncio.sleep(60)  # Retry after 1 minute


# Initialization function
async def initialize_pipeline(connection_string: str) -> TrainingOrchestrator:
    """Initialize the AI training pipeline"""
    logger.info("üîß Initializing AI Training Pipeline")
    
    # Configure SSL if unified certificate exists
    if os.path.exists(AITrainingPipelineConfig.SSL_CERT_PATH):
        logger.info(f"üîí Using unified certificate: {AITrainingPipelineConfig.SSL_CERT_PATH}")
        os.environ['REQUESTS_CA_BUNDLE'] = AITrainingPipelineConfig.SSL_CERT_PATH
        os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = AITrainingPipelineConfig.SSL_CERT_PATH
    
    orchestrator = TrainingOrchestrator(connection_string)
    
    logger.info("‚úÖ Pipeline initialization complete")
    return orchestrator


# Usage example
if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("NetworkBuster AI Training Pipeline")
    logger.info("=" * 60)
    
    # Load xkcd story knowledge
    logger.info("\nüìö Loading xkcd Story Knowledge...")
    story_loader = XKCDStoryContextLoader()
    story_knowledge = story_loader.get_story_context_for_training(max_stories=5)
    
    if story_knowledge:
        logger.info("‚úÖ xkcd stories loaded successfully!")
        logger.info("\n=== Sample Knowledge Preview ===")
        # Print first 500 characters
        preview = story_knowledge[:500] + "..." if len(story_knowledge) > 500 else story_knowledge
        print(preview)
    else:
        logger.warning("‚ö†Ô∏è  No xkcd stories available")
    
    # Configuration
    connection_string = os.getenv(
        'AZURE_STORAGE_CONNECTION_STRING',
        'DefaultEndpointsProtocol=https;AccountName=xxx;AccountKey=xxx;EndpointSuffix=core.windows.net'
    )
    
    # Show available models
    logger.info("\nüìä Available Training Models:")
    for model_key, model_config in AITrainingPipelineConfig.TRAINING_CONFIGS.items():
        logger.info(f"  ‚Ä¢ {model_key}: {model_config['type']}")
    
    # Initialize (don't run in main scope)
    logger.info("\n‚úÖ Pipeline ready for async execution")
    logger.info("Use: await initialize_pipeline(connection_string)")